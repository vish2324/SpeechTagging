{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Code_ML.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "w-iNyUhfCB96"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "xdE-pJ9d6zdy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gkXLsvrwD3NY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import copy\n",
        "from operator import itemgetter\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_2n2P3oa2Tb0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Local\n",
        "#file_path = \"Term 6/01.112 - ML/Project/EN/\" \n",
        "\n",
        "\n",
        "#Hosted\n",
        "file_path = \"/content/drive/My Drive/Machine Learning project/FR/\"\n",
        "\n",
        "train_file_name = \"train\"\n",
        "test_file_in = \"dev.in\"\n",
        "p2_test_file_out = \"dev.p2.out\"\n",
        "p3_test_file_out = \"dev.p3.out\"\n",
        "p4_test_file_out = \"dev.p4.out\"\n",
        "p5_test_file_out = \"dev.p5.out\"\n",
        "blind_test_file_in = \"test.in\"  #CHANGE TO test2.in\n",
        "blind_test_file_out = \"test.p5.out\"  #CHANGE TO test2.p5.out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RIYeZxH54baK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Part 2"
      ]
    },
    {
      "metadata": {
        "id": "LZmN-5gQOJYw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Read and parse train file"
      ]
    },
    {
      "metadata": {
        "id": "6xwKxLA_9ePC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "file = open(file_path + train_file_name,\"r\", encoding='utf8')  #Open training file\n",
        "l = [line.rstrip('\\n').split(\" \") for line in file]  #Remove newline character and split words and tags\n",
        "\n",
        "#Adds Stop and Start tags for each sentence\n",
        "m = [['__Start__',\"Start\"]]  #First Start tag\n",
        " \n",
        "for pair in l:\n",
        "  if len(pair) == 1:\n",
        "    m.append(['__Stop__',\"Stop\"])\n",
        "    m.append(['__Start__',\"Start\"])\n",
        "  else:\n",
        "    m.append(pair)\n",
        "\n",
        "m =  m[:-1]  #Drops last extra Start tag\n",
        "\n",
        "df = pd.DataFrame(m)  #convert Array to pandas DataFrame"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "APhH8IjI9vrw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tags = df[1].unique()  #Obtain all unique tags in train\n",
        "words = df[0].unique()  #Obtain all unique words in train\n",
        "\n",
        "tagc_dict = df[1].value_counts().to_dict()  #Dictionary of tag counts\n",
        "\n",
        "#Sorting tags in descending order\n",
        "tags = np.sort(tags)  \n",
        "tags = tags[::-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dDMVnzAWOOQn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Count words"
      ]
    },
    {
      "metadata": {
        "id": "DSpWxfwh9xR8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Creating a dict with count of words dataframe\n",
        "#Dict -> key = tags, values = count of words df for tag\n",
        "#Df -> column name = tag, index = word\n",
        "\n",
        "wordc_dictdf = {}  #Dictionary with word count dataframe\n",
        "for tag in tags:\n",
        "    temp_df = df.loc[df[1] == tag][0].value_counts().to_frame()\n",
        "    temp_df.columns = [tag]\n",
        "    wordc_dictdf[tag] = temp_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s0-n5KIzOQTh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Calculate emission parameter"
      ]
    },
    {
      "metadata": {
        "id": "mfZb_2KFggI5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Keeps count of each word and creates new dict for emission parameter calculation\n",
        "emm_dictdf = copy.deepcopy(wordc_dictdf)  \n",
        "\n",
        "k=1\n",
        "#Calculating emission parameter for each word\n",
        "for tag in tags:\n",
        "  emm_dictdf[tag] /= (tagc_dict[tag]+k) # Count (u -> v)/ Count (u)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RsR7vG5MOS2c",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Functions to obtain tag from word"
      ]
    },
    {
      "metadata": {
        "id": "O1hXgiTihiEw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Smoothing function if test word not in train\n",
        "def smoothing(tag,k=1):  # k=1 - Hyperparameter for smoothing\n",
        "  return k/(tagc_dict[tag]+k)\n",
        "\n",
        "#Returns emission parameter value of a word given a tag\n",
        "def emission(word,tag):\n",
        "  if word in words:\n",
        "    return emm_dictdf[tag][tag].get(word,0)\n",
        "  else:\n",
        "    return smoothing(tag)\n",
        "\n",
        "#Find tag with maximum emission parameter for a word\n",
        "def argmax_em(word):\n",
        "  tmpemm_dict={}\n",
        "  for tag in tags:\n",
        "    tmpemm_dict.update({tag:emission(word,tag)})\n",
        "  return max(tmpemm_dict, key = lambda x: tmpemm_dict.get(x))  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b_2jueRBOXL9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "read dev.out and write dev.p2.out"
      ]
    },
    {
      "metadata": {
        "id": "CMRxrM4Z1IRX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Reading test file and predicting tag given word\n",
        "test_file = open(file_path+test_file_in,\"r\", encoding='utf8')  #Open training file\n",
        "l = [line.rstrip('\\n').split(\" \") for line in test_file]  #Remove newline character and split words and tags\n",
        "\n",
        "out = []\n",
        "for word in l:\n",
        "  if word[0] == '':\n",
        "    out.append('\\n')\n",
        "  else:\n",
        "    out.append(word[0] + \" \" + argmax_em(word[0]) + '\\n')\n",
        "    \n",
        "    \n",
        "with open(file_path + p2_test_file_out, 'w', encoding='utf8') as f:\n",
        "  for item in out:\n",
        "    f.write('%s' %(item))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6_exOU-_eNnV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Part 3"
      ]
    },
    {
      "metadata": {
        "id": "R6MIS2SEOcDN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Transition parameter calculation"
      ]
    },
    {
      "metadata": {
        "id": "St7FDSova6bO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Creating empty dataframe to hold all transition parameter values\n",
        "#u is an array to store possible tags except stop\n",
        "u = [\"Start\"]\n",
        "u += [tag for tag in tags if (tag != 'Stop' and tag != 'Start')]\n",
        "\n",
        "v = u[1:] + ['Stop']\n",
        "v = np.asarray(v)\n",
        "\n",
        "u= np.asarray(u)\n",
        "\n",
        "tran_df = pd.DataFrame(0,index=v, columns=u)\n",
        "\n",
        "#Calculating count from u to v\n",
        "for num in range(len(df[1])):\n",
        "  if (df[1][num] != 'Stop'):\n",
        "    tran_df[df[1][num]][df[1][num+1]] +=1\n",
        "    \n",
        "#Calculating transition parameter    \n",
        "for tag in u:\n",
        "  tran_df[tag] /= tagc_dict[tag]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jS3dY-yuNrED",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "read and parse dev.in for Part 3"
      ]
    },
    {
      "metadata": {
        "id": "0HLbsdKY3f7D",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Preparing input file for Viterbi Calculation\n",
        "\n",
        "test_file = open(file_path+test_file_in,\"r\", encoding='utf8')  #Open training file\n",
        "l = [line.rstrip('\\n').split(\" \") for line in test_file]  #Remove newline character and split words and tags\n",
        "\n",
        "m_test = ['__Start__']  #First Start tag\n",
        "\n",
        "#Put Start and Stop tag placeholders and track index of each placeholder\n",
        "indexing = [0]\n",
        "for i in range(len(l)):\n",
        "  if l[i][0] == '':\n",
        "    m_test = m_test + ['__Stop__']\n",
        "    indexing += [len(m_test)-1]\n",
        "    m_test = m_test + ['__Start__']\n",
        "    indexing += [len(m_test)-1]\n",
        "  else:\n",
        "    m_test = m_test + l[i]\n",
        "    \n",
        "#Clean up extra placeholders\n",
        "m_test =  m_test[:-1]  #Drops last extra Start tag\n",
        "indexing = indexing[:-1]  #Drops last extra Start tag\n",
        "\n",
        "#Creating easy sequencing index to obtain each word sequence\n",
        "index = [indexing[0::2],indexing[1::2]]\n",
        "\n",
        "#Store each sequence of words for Viterbi Algorithm\n",
        "sequence = []\n",
        "for i in range(len(index[0])):\n",
        "  sequence += [m_test[index[0][i]:index[1][i]:]]\n",
        "  \n",
        "# sequence is a colelction of the tweets to be passed to viterbi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ioKnYdgBNj_q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Viterbi Algorithm Implementation\n",
        "\n",
        "And \n",
        "\n",
        "write dev.p3.out"
      ]
    },
    {
      "metadata": {
        "id": "BrUuN0_2hHRi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "output = []\n",
        "\n",
        "for s in range(len(sequence)):\n",
        "  n = len(sequence[s])\n",
        "  \n",
        "  #Empty Dataframe\n",
        "  val_pi = pd.DataFrame(index=range(n),columns=u)\n",
        "  tag_pi = [None] * (n+1)\n",
        "  \n",
        "  #Base Case\n",
        "  for v_tag in tags[tags != 'Stop']:\n",
        "    if v_tag == 'Start':\n",
        "      val_pi[v_tag][0] = 1\n",
        "    else:\n",
        "      val_pi[v_tag][0] = 0\n",
        "  \n",
        "  #Recursive Case\n",
        "  for k in range(1,n):  \n",
        "    for v_tag in v[v != 'Stop']:\n",
        "      emm_v = emission(sequence[s][k],v_tag)\n",
        "      if emm_v != 0:\n",
        "        temp = []  \n",
        "        for u_tag in u:\n",
        "          temp.append(val_pi[u_tag][k-1] * tran_df[u_tag][v_tag] * emm_v)\n",
        "        max_val = max(filter(lambda x: not np.isnan(x), temp))\n",
        "        val_pi[v_tag][k] = max_val\n",
        "    if val_pi.sum(axis=1)[k] != 0:        \n",
        "      val_pi.loc[k,:] = val_pi.loc[k,:].div(val_pi.sum(axis=1)[k], axis=0)    \n",
        "  \n",
        "  #Finding Tags\n",
        "  tag_pi[n] = 'Stop'\n",
        "  for i in range(n-1,-1,-1):\n",
        "    temp=[]\n",
        "    for v_tag in tags[tags != 'Stop']:\n",
        "      temp.append([v_tag,val_pi[v_tag][i] * tran_df[v_tag][tag_pi[i+1]]])\n",
        "    tag_pi[i] = max(filter(lambda x: not np.isnan(x[1]), temp), key=itemgetter(0))[0]\n",
        "  \n",
        "  #Create output List\n",
        "  for i in range(1,len(sequence[s])):\n",
        "    output.append(sequence[s][i] + \" \" +tag_pi[i] + '\\n')\n",
        "  output.append('\\n')\n",
        "  \n",
        "\n",
        "#Write to File  \n",
        "with open(file_path + p3_test_file_out, 'w', encoding='utf8') as f:\n",
        "  for item in output:\n",
        "    f.write('%s' %(item))  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BSm8B-Kw4WPQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Part 4 "
      ]
    },
    {
      "metadata": {
        "id": "WOI5jqWTOkcW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Second order transition parameter calculation"
      ]
    },
    {
      "metadata": {
        "id": "lNpjtFrb7hXI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Second order assumption transition parameter calculation\n",
        "\n",
        "#DataFrame to represent tags in y(i-2) and y(i-1) as index\n",
        "tran_df2 = pd.DataFrame(index=u, columns=u)\n",
        "\n",
        "#DataFrame to represent tags in y(i) as index\n",
        "initial = pd.DataFrame(columns = v)\n",
        "initial.loc[0] = 0.0\n",
        "\n",
        "#Deep copy of y(i) DataFrame\n",
        "for tag1 in u:\n",
        "  for tag2 in u:\n",
        "    tran_df2[tag1][tag2] = copy.deepcopy(initial)\n",
        "    \n",
        "#Count( (y(i-2), y(i-1)); y(i) )    \n",
        "for i in range(len(df[1])-1):\n",
        "  j = i if df[1][i] == 'Start' else i-1\n",
        "  if df[1][j] != 'Stop' and df[1][i] != 'Stop': \n",
        "    tran_df2[df[1][j]][df[1][i]][df[1][i+1]] += 1.0\n",
        "\n",
        "#Divide by Count( (y(i-2), y(i-2)) )\n",
        "for tag1 in u:\n",
        "  for tag2 in u:\n",
        "    sum_row = tran_df2[tag1][tag2].sum(axis=1)[0]\n",
        "    if sum_row != 0:\n",
        "      tran_df2[tag1][tag2] /= sum_row    \n",
        "\n",
        "#a(t, u, v) => a(y(i-2), y(i-1), y(i)) => tran_df2[y(i-2)][y(i-1)][y(i)][0]\n",
        "#tran_df2[u][u][v][0]      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XQGC9McQNcjr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Second order Viterbi Algorithm Implementation\n",
        "\n",
        "And\n",
        "\n",
        "write dev.p4.out "
      ]
    },
    {
      "metadata": {
        "id": "UaTOvuV2Llpz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "output = []\n",
        "for s in range(len(sequence)):\n",
        "\n",
        "  n = len(sequence[s]) # n=16 | range(n) = 0,1,2,...,15\n",
        "    \n",
        "  #Empty Dataframe\n",
        "  val_pi2 = pd.DataFrame(index=u,columns=range(n+1))\n",
        "\n",
        "  initial = pd.DataFrame(columns = tags)\n",
        "  initial.loc[0] = None\n",
        "\n",
        "  for k in range(n+1):\n",
        "    for t_tag in u:\n",
        "      val_pi2[k][t_tag] = copy.deepcopy(initial)\n",
        "      \n",
        "  #pi(k, (u,v)) => val_pi2[k][u][v]\n",
        "  #val_pi2[range(n+1)][u][tags][0]\n",
        "\n",
        "\n",
        "  #Base Case\n",
        "  for k in range(2):\n",
        "    for u_tag in u:\n",
        "      for v_tag in tags:\n",
        "        if k == 0:\n",
        "          if u_tag == 'Start' and v_tag == 'Start':\n",
        "            val_pi2[k][u_tag][v_tag][0] = 1.0\n",
        "          else:\n",
        "            val_pi2[k][u_tag][v_tag][0] = 0.0\n",
        "        else:\n",
        "          if u_tag == 'Start' and v_tag != 'Start':\n",
        "            val_pi2[k][u_tag][v_tag][0] = tran_df2['Start']['Start'][v_tag][0] * emission(sequence[s][k], v_tag)\n",
        "          else:\n",
        "            val_pi2[k][u_tag][v_tag][0] = 0.0       \n",
        "            \n",
        "  #val_pi2 [range(n+1)][u][tags][0]\n",
        "  #tran_df2[    u     ][u][ v  ][0]\n",
        "\n",
        "\n",
        "  #Recursive Case\n",
        "  for k in range(2,n):  #2,3,...,15\n",
        "    for v_tag in v[v != 'Stop']:\n",
        "      emm_v = emission(sequence[s][k],v_tag)\n",
        "      if emm_v != 0:\n",
        "        \n",
        "        for u_tag in u[u != 'Start']:      \n",
        "          temp = []  \n",
        "          for t_tag in u:          \n",
        "            temp.append(val_pi2[k-1][t_tag][u_tag][0] * tran_df2[t_tag][u_tag][v_tag][0] * emm_v)\n",
        "\n",
        "          filtered = list(filter(lambda x: not np.isnan(x),temp))\n",
        "          max_val = float('NaN') if len(filtered) == 0 else max(filtered)\n",
        "          val_pi2[k][u_tag][v_tag][0] = max_val\n",
        "    \n",
        "    for u_tag in u:\n",
        "      val_sum = val_pi2[k][u_tag].sum(axis=1)[0]\n",
        "      if val_sum != 0:  \n",
        "        val_pi2[k][u_tag].loc[:] = val_pi2[k][u_tag].loc[:].div(val_sum)                    \n",
        "        \n",
        "\n",
        "  #Final Transition Case      \n",
        "  for u_tag in u[u != 'Start']:\n",
        "    temp = []\n",
        "    for t_tag in u[u != 'Start']:\n",
        "      temp.append(val_pi2[n-1][t_tag][u_tag][0] * tran_df2[t_tag][u_tag]['Stop'][0])\n",
        "    filtered = list(filter(lambda x: not np.isnan(x), temp))\n",
        "    max_val = float('NaN') if len(filtered) == 0 else max(filtered)\n",
        "    val_pi2[n][u_tag]['Stop'] = max_val           \n",
        "    \n",
        "\n",
        "  #Finding Tags  \n",
        "  tag_pi2 = [None] * (n+1)  \n",
        "\n",
        "  tag_pi2[n] = 'Stop'\n",
        "  tag_pi2[0] = 'Start'\n",
        "\n",
        "  temp=[]\n",
        "  for u_tag in u[u != 'Start']:\n",
        "    temp.append([u_tag, val_pi2[n][u_tag]['Stop'][0]])\n",
        "  tag_pi2[n-1] = max(filter(lambda x: not np.isnan(x[1]), temp), key=itemgetter(0))[0]\n",
        "\n",
        "  for k in range(n-2,0,-1):\n",
        "    temp=[]\n",
        "    for t_tag in u[u!='Start']:\n",
        "      temp.append([ t_tag, val_pi2[k+1][t_tag][tag_pi2[k+1]][0] * tran_df2[t_tag][tag_pi2[k+1]][tag_pi2[k+2]][0] ])\n",
        "    tag_pi2[k] = max(filter(lambda x: not np.isnan(x[1]), temp), key=itemgetter(0))[0]    \n",
        "    \n",
        "    \n",
        "  #Create output List\n",
        "  for i in range(1,len(sequence[s])):\n",
        "    output.append(sequence[s][i] + \" \" + tag_pi2[i] + '\\n')\n",
        "  output.append('\\n')  \n",
        "  \n",
        "\n",
        "#Write to File  \n",
        "with open(file_path + p4_test_file_out, 'w') as f:\n",
        "  for item in output:\n",
        "    f.write('%s' %(item))  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q-efvrij6Ja_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Part 5"
      ]
    },
    {
      "metadata": {
        "id": "qbyBFNlbOpcV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Improvement"
      ]
    },
    {
      "metadata": {
        "id": "UPCFnUas6L-R",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "my_out = open(file_path + p3_test_file_out, 'r', encoding='utf8')\n",
        "actual_out = open(file_path + 'dev.out', 'r', encoding='utf8')\n",
        "\n",
        "pred_out = [line.rstrip('\\n').split(\" \") for line in my_out]\n",
        "compare_out = [line.rstrip('\\n').split(\" \") for line in actual_out]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PjJ-nprgTl_h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for i in range(len(pred_out)):  \n",
        "  if len(pred_out[i]) != 1: #Ignore all the blank lines\n",
        "    if pred_out[i][1] != compare_out[i][1]: #If predicted not same as actual\n",
        "      \n",
        "      #Update emission parameters \n",
        "      #Word in train list of words, increase emission probability of the correct tag\n",
        "      if pred_out[i][0] in words: \n",
        "        if pred_out[i][0] in emm_dictdf[compare_out[i][1]].index:\n",
        "          emm_dictdf[compare_out[i][1]][compare_out[i][1]][pred_out[i][0]] += 2\n",
        "        else:\n",
        "          emm_dictdf[compare_out[i][1]].loc[pred_out[i][0]] = 2\n",
        "      #Word not in train list of words, add to list of words and set emission probability for correct tag\n",
        "      else: \n",
        "        emm_dictdf[compare_out[i][1]][compare_out[i][1]][pred_out[i][0]] = 10\n",
        "        np.append(words,pred_out[i][0])\n",
        "        #tagc_dict[compare_out[i][1]] -= 1\n",
        "        \n",
        "      #Update Part 3 transition parameters\n",
        "      if i == 0 or len(pred_out[i-1]) == 1:\n",
        "        tran_df['Start'][compare_out[i][1]] = tran_df['Start'][compare_out[i][1]] + 2\n",
        "      else:\n",
        "        tran_df[compare_out[i-1][1]][compare_out[i][1]] = tran_df[compare_out[i-1][1]][compare_out[i][1]] + 2\n",
        "               "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MzjXzbtsP8wx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "HMM Calculation\n",
        "\n",
        "And\n",
        "\n",
        "write dev.p5.out"
      ]
    },
    {
      "metadata": {
        "id": "lrCTTbHeO6U8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "output = []\n",
        "\n",
        "for s in range(len(sequence)):\n",
        "  n = len(sequence[s])\n",
        "  \n",
        "  #Empty Dataframe\n",
        "  val_pi = pd.DataFrame(index=range(n),columns=u)\n",
        "  tag_pi = [None] * (n+1)\n",
        "  \n",
        "  #Base Case\n",
        "  for v_tag in tags[tags != 'Stop']:\n",
        "    if v_tag == 'Start':\n",
        "      val_pi[v_tag][0] = 1\n",
        "    else:\n",
        "      val_pi[v_tag][0] = 0\n",
        "  \n",
        "  #Recursive Case\n",
        "  for k in range(1,n):  \n",
        "    for v_tag in v[v != 'Stop']:\n",
        "      emm_v = emission(sequence[s][k],v_tag)\n",
        "      if emm_v != 0:\n",
        "        temp = []  \n",
        "        for u_tag in u:\n",
        "          temp.append(val_pi[u_tag][k-1] * tran_df[u_tag][v_tag] * emm_v)\n",
        "        max_val = max(filter(lambda x: not np.isnan(x), temp))\n",
        "        val_pi[v_tag][k] = max_val\n",
        "    if val_pi.sum(axis=1)[k] != 0:        \n",
        "      val_pi.loc[k,:] = val_pi.loc[k,:].div(val_pi.sum(axis=1)[k], axis=0)    \n",
        "  \n",
        "  #Finding Tags\n",
        "  tag_pi[n] = 'Stop'\n",
        "  for i in range(n-1,-1,-1):\n",
        "    temp=[]\n",
        "    for v_tag in tags[tags != 'Stop']:\n",
        "      temp.append([v_tag,val_pi[v_tag][i] * tran_df[v_tag][tag_pi[i+1]]])\n",
        "    tag_pi[i] = max(filter(lambda x: not np.isnan(x[1]), temp), key=itemgetter(0))[0]\n",
        "  \n",
        "  #Create output List\n",
        "  for i in range(1,len(sequence[s])):\n",
        "    output.append(sequence[s][i] + \" \" +tag_pi[i] + '\\n')\n",
        "  output.append('\\n')\n",
        "  \n",
        "\n",
        "#Write to File  \n",
        "with open(file_path + p5_test_file_out, 'w', encoding='utf8') as f:\n",
        "  for item in output:\n",
        "    f.write('%s' %(item))  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x2-jdwAzNBCg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "test.in/test2.in parsing"
      ]
    },
    {
      "metadata": {
        "id": "VjHtXDlYMXy2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Preparing input file for Viterbi Calculation\n",
        "\n",
        "test_file = open(file_path+blind_test_file_in,\"r\", encoding='utf8')  #Open training file\n",
        "l = [line.rstrip('\\n').split(\" \") for line in test_file]  #Remove newline character and split words and tags\n",
        "\n",
        "m_test = ['__Start__']  #First Start tag\n",
        "\n",
        "#Put Start and Stop tag placeholders and track index of each placeholder\n",
        "indexing = [0]\n",
        "for i in range(len(l)):\n",
        "  if l[i][0] == '':\n",
        "    m_test = m_test + ['__Stop__']\n",
        "    indexing += [len(m_test)-1]\n",
        "    m_test = m_test + ['__Start__']\n",
        "    indexing += [len(m_test)-1]\n",
        "  else:\n",
        "    m_test = m_test + l[i]\n",
        "    \n",
        "#Clean up extra placeholders\n",
        "m_test =  m_test[:-1]  #Drops last extra Start tag\n",
        "indexing = indexing[:-1]  #Drops last extra Start tag\n",
        "\n",
        "#Creating easy sequencing index to obtain each word sequence\n",
        "index = [indexing[0::2],indexing[1::2]]\n",
        "\n",
        "#Store each sequence of words for Viterbi Algorithm\n",
        "sequence = []\n",
        "for i in range(len(index[0])):\n",
        "  sequence += [m_test[index[0][i]:index[1][i]:]]\n",
        "  \n",
        "# sequence is a collection of the tweets to be passed to viterbi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "80QV1KRdN4X6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "HMM calculation\n",
        "\n",
        "And\n",
        "\n",
        "write test.p5.out/test2.p5.out"
      ]
    },
    {
      "metadata": {
        "id": "TZV1lyGCdsBl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "output = []\n",
        "\n",
        "for s in range(len(sequence)):\n",
        "  n = len(sequence[s])\n",
        "  \n",
        "  #Empty Dataframe\n",
        "  val_pi = pd.DataFrame(index=range(n),columns=u)\n",
        "  tag_pi = [None] * (n+1)\n",
        "  \n",
        "  #Base Case\n",
        "  for v_tag in tags[tags != 'Stop']:\n",
        "    if v_tag == 'Start':\n",
        "      val_pi[v_tag][0] = 1\n",
        "    else:\n",
        "      val_pi[v_tag][0] = 0\n",
        "  \n",
        "  #Recursive Case\n",
        "  for k in range(1,n):  \n",
        "    for v_tag in v[v != 'Stop']:\n",
        "      emm_v = emission(sequence[s][k],v_tag)\n",
        "      if emm_v != 0:\n",
        "        temp = []  \n",
        "        for u_tag in u:\n",
        "          temp.append(val_pi[u_tag][k-1] * tran_df[u_tag][v_tag] * emm_v)\n",
        "        max_val = max(filter(lambda x: not np.isnan(x), temp))\n",
        "        val_pi[v_tag][k] = max_val\n",
        "    if val_pi.sum(axis=1)[k] != 0:        \n",
        "      val_pi.loc[k,:] = val_pi.loc[k,:].div(val_pi.sum(axis=1)[k], axis=0)    \n",
        "  \n",
        "  #Finding Tags\n",
        "  tag_pi[n] = 'Stop'\n",
        "  for i in range(n-1,-1,-1):\n",
        "    temp=[]\n",
        "    for v_tag in tags[tags != 'Stop']:\n",
        "      temp.append([v_tag,val_pi[v_tag][i] * tran_df[v_tag][tag_pi[i+1]]])\n",
        "    tag_pi[i] = max(filter(lambda x: not np.isnan(x[1]), temp), key=itemgetter(0))[0]\n",
        "  \n",
        "  #Create output List\n",
        "  for i in range(1,len(sequence[s])):\n",
        "    output.append(sequence[s][i] + \" \" +tag_pi[i] + '\\n')\n",
        "  output.append('\\n')\n",
        "  \n",
        "\n",
        "#Write to File  \n",
        "with open(file_path + blind_test_file_out, 'w', encoding='utf8') as f:\n",
        "  for item in output:\n",
        "    f.write('%s' %(item))  "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}